{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOINW0vvAPEqoazshYAckGu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djprojecthub/handson-llm/blob/main/Chapter_1_Generate_First_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7531b840"
      },
      "source": [
        "This cell imports necessary classes from the `transformers` library and loads a pre-trained large language model (LLM) and its corresponding tokenizer.\n",
        "\n",
        "*   `AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", ...)`: Loads the 'Phi-3-mini-4k-instruct' model, which is a causal language model suitable for instruction-following tasks.\n",
        "    *   `device_map=\"auto\"`: Automatically determines the best device (CPU or GPU) to load the model onto.\n",
        "    *   `torch_dtype=\"auto\"`: Automatically selects the appropriate data type for PyTorch tensors, optimizing for performance.\n",
        "*   `AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")`: Loads the tokenizer associated with the 'Phi-3-mini-4k-instruct' model. The tokenizer is essential for converting text into a format the model can understand and vice-versa."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "38qRHMgEd8uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbd1f113"
      },
      "source": [
        "This cell sets up a text generation pipeline using the loaded model and tokenizer.\n",
        "\n",
        "*   `from transformers import pipeline`: Imports the `pipeline` function, a high-level API for using models for various tasks.\n",
        "*   `generator = pipeline(\"text-generation\", ...)`: Initializes a text generation pipeline.\n",
        "    *   `model=model`: Specifies the pre-trained language model to use for generation.\n",
        "    *   `tokenizer=tokenizer`: Specifies the tokenizer to use for processing input and output text.\n",
        "    *   `return_full_text=False`: Ensures that only the newly generated text is returned, not the input prompt concatenated with the generated text.\n",
        "    *   `max_new_tokens=500`: Sets the maximum number of new tokens (words/subwords) the model can generate.\n",
        "    *   `progress_bar=False`: Disables the progress bar during generation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        "    progress_bar=False,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "YsLOoiI8ZvrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a53c39a"
      },
      "source": [
        "This cell defines an input message for the model, generates text using the pipeline, and prints the output.\n",
        "\n",
        "*   `message = [{ \"role\":\"user\", \"content\":\"Create a funny joke about chickens.\" }]`: Defines the input message in a chat-like format, specifying the role as \"user\" and the content of the request.\n",
        "*   `output = generator(message)[0][\"generated_text\"]`: Calls the text generation pipeline with the defined message. The output is a list of dictionaries, so `[0][\"generated_text\"]` extracts the actual generated text from the first (and usually only) result.\n",
        "*   `print(output)`: Prints the generated joke to the console."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [{\n",
        "    \"role\":\"user\",\n",
        "    \"content\":\"Create a funny joke about chickens.\"\n",
        "}]\n",
        "\n",
        "output=generator(message)[0][\"generated_text\"]\n",
        "print(output)"
      ],
      "metadata": {
        "id": "s4sU6jnfatxS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}